{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel Reducing #\n",
    "\n",
    "Usually, classification networks are made of two parts: the convolutionnal network part at first, to do the heavy image processing calculations, and some fully connected layers at the end, to get to the wanted number of outputs. Fully connected layers are a critical part of the network, since those can represent a considerable number of parameters. \n",
    "\n",
    "Let's take an example to put things more into perspective. The code here-below creates two different (but similar models). Those models take as inputs 128x128 grayscale images and classify them in one of ten classes.\\\n",
    "The first model contains 3 convolutionnal layers and 1 dense layer.\\\n",
    "The second model contains 4 convolutional layers (the first 3 are the same as the first model) and 1 dense layer.\\\n",
    "Let's run the code and take a look at the number of parameters these 2 models have :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 128, 128, 32)      320       \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 128, 128, 16)      4624      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 128, 128, 8)       1160      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 131072)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1310730   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,316,834\n",
      "Trainable params: 1,316,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 128, 128, 32)      320       \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 128, 128, 16)      4624      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 128, 128, 8)       1160      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 128, 128, 4)       292       \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 65536)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                655370    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 661,766\n",
      "Trainable params: 661,766\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def no_channel_reducing_example(input_shape=(128, 128, 1)):\n",
    "    inputs = Input(input_shape)\n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    x = Conv2D(16, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(8, (3, 3), padding='same', activation='relu')(x)\n",
    "    x1 = Conv2D(4, (3, 3), padding='same', activation='relu')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    x1 = Flatten()(x1)\n",
    "    x1 = Dense(10, activation='softmax')(x1)\n",
    "    model1 = Model(inputs=inputs, outputs=x1)\n",
    "\n",
    "    model.summary()\n",
    "    model1.summary()\n",
    "\n",
    "no_channel_reducing_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice ? The second model, despite having strictly more layers, has less than half the number of parameters the first model has !\\\n",
    "How did this happen ? \\\n",
    "Let's make the calculations by ourselves. Pay attention to the output of the flatten layer:\\\n",
    "A vector of size 131,072 for the first model\\\n",
    "A vector of size 65,536 for the second model\\\n",
    "This is completely normal: all the flatten layer does is convert any shape of input into a vector by putting each feature one after the other. The difference in vector size between the 2 outputs of the 2 flatten layers come from the number of channels the outputs of the convolutionnal layer right before the flatten layer have: 8 channels for the first model and 4 for the second.\\\n",
    "If we do the math, we get the right result:\\\n",
    "128x128x8 = 131,072\\\n",
    "128x128x4 = 65,536\\\n",
    "If we look more closely, we notice that the layers containing the vast majority of the parameters are for both models the fully coonected layers.\n",
    "This can be explained by what we just showed. Indeed, the number of parameters of a fully connected layer are:\\\n",
    "input_size*output_size+output_size\\\n",
    "Because all a fully connected layers does is a matrix multiplication and a vector addition (weigths x inputs + biases). The size of the weight matrix has to be output_size*input_size and the size of the bias vector has to be output_size.\n",
    "If we do the math for the 2 models, we obtain the same results as tensorflow! \\\n",
    "This is where we can guess what the channel reduction technique is. It consists of adding layers with a reduced amount of filters right before the flatten layers.\\\n",
    "What we can wonder next is, does unsing this technique reduces the accuracy of a model ? It seems it does, since there are all of a sudden much less parameters in the model...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
